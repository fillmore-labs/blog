{"/about/":{"data":{"":"I embarked on my journey in programming on a humble 6502 Machine, equipped with just 1K of RAM. Eventually, I delved into the intricate task of reverse engineering and enhancing the firmware of a 5¼\" floppy disk drive.\nAfter writing a lot of C++ and Java and studying Mathematics and Computer Science I worked in mobile development in an architect role for a while but moved to backend and systems architecture since then.\nBesides software architecture and archaeology my expertise is in event-based systems, domain-driven design (DDD) and streaming data processing. I am also leading project teams, mentoring developers and care a lot about testing and quality assurance.\nLast but not least I recognize the human aspect of software development and believe that software architecture is sometimes more of an art than a craft."},"title":"About Me"},"/posts/concurrency-bugs-1/":{"data":{"":"","errors-observed-in-real-world-usage#Errors Observed in Real-world Usage":"In “Problems With Concurrency” I mentioned that I see concurrency issues a lot. Let’s look at something I recently found in production:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 package main func task() ([]int, error) { // We want to calculate all results in parallel. So we create a wait group with 3 elements. We will call wg.Done() // after each goroutine is finished. We also create a mutex to lock the result while writing to it. We also create // an error channel to collect errors from the goroutines. var wg sync.WaitGroup var mu sync.Mutex errs := make(chan error) // Create result result := make([]int ,3) // We create a wait group with 3 elements. We will call wg.Done() after each goroutine is finished. wg.Add(3) // We start all goroutines. go setResult(result, 0, \u0026wg, \u0026mu, errs) go setResult(result, 1, \u0026wg, \u0026mu, errs) go setResult(result, 2, \u0026wg, \u0026mu, errs) wg.Wait() // Handle errors close(errs) for err := range errs { return nil, err } return result, nil } func setResult(result []int, i int, wg *sync.WaitGroup, mu *sync.Mutex, errs chan\u003c- error) { defer wg.Done() r, err := calculate(i) if err != nil { errs \u003c- err return } mu.Lock() result[i] = r mu.Unlock() } func calculate(i int) (int, error) { switch i { case 0: return 1, nil case 11: return 2, nil case 2: return 3, nil } return 0, errTest } var errTest = errors.New(\"test\") Try it on the Go Playground.\nThis code is pretty similar to the one in “Problems With Concurrency” before it was modified in the blog. It makes nearly that same mistake and deadlocks on error conditions. Obviously[^1], the fix is not to make this function more complex and use errs := make(chan error, 3).\nWhat Are the Issues? This code very nicely demonstrates what you get when you start with concurrent code without thinking about the design.\nLet first fix it, then analyze its problems. The task is divided into three independent calculations, where failure in any one of them results in the failure of the entire task. The synchronous version would be:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func task() ([]int, error) { const results = 3 result := make([]int, results) var err error for i := 0; i \u003c results; i++ { result[i], err = calculate(i) if err != nil { break } } return result, err } Where we see immediately that we don’t need the mutex, since the results don’t overlap. Transforming into a parallel version gives us:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import \"golang.org/x/sync/errgroup\" func task() ([]int, error) { const results = 3 result := make([]int, results) var g errgroup.Group for i := 0; i \u003c results; i++ { r := \u0026result[i] // Added for clarity g.Go(func() (err error) { *r, err = calculate(i) return err }) } return result, g.Wait() } Given that this task appears straightforward, why are there bugs in the production code?\nThe comments provide valuable insights, beginning with the directive to “calculate … in parallel”, but without considering the task’s purpose and communication methods. Consequently, it establishes … as synchronization points, along with … and …\nThe code wasn’t initially designed with correctness as the primary concern. Instead, it began as an asynchronous version, employing go as a substitute for subroutine calls, with fixes being implemented reactively as problems arose. Regrettably, this pattern is frequently observed among junior Go developers.","summary#Summary":"In designing asynchronous programs, it is often better to begin with a synchronous, correct version. This initial approach might even suffice in terms of performance, with parallelism introduced by a calling function, such as operating within one of multiple web requests. Additionally, it’s worth noting that concurrency doesn’t always need to be that extremely fine-grained, especially considering that the number of CPUs in a machine is limited.\nMoreover, channels and synchronization points should be purposefully integrated, not employed as mere necessities. The need for error checking shouldn’t be retrospectively fixed by a channel with slapped-on synchronization primitives."},"title":"Concurrency Bugs"},"/posts/concurrency-bugs-2/":{"data":{"":"","more-real-world-usage#More Real-world Usage":"Datadog has excellent software engineers. Nevertheless, it’s easy to find code with race conditions. Let’s examine a simplified version of waitForConfigsFromAD:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 package main import ( \"fmt\" \"runtime\" \"time\" \"go.uber.org/atomic\" ) type Config struct{} type Component interface { AddScheduler(s func([]Config)) } func waitForConfigsFromAD(discoveryMinInstances int, ac Component) (configs []Config, returnErr error) { configChan := make(chan Config) // signal to the scheduler when we are no longer waiting, so we do not continue // to push items to configChan waiting := atomic.NewBool(true) defer func() { waiting.Store(false) // ..and drain any message currently pending in the channel select { case \u003c-configChan: default: } }() // add the scheduler in a goroutine, since it will schedule any \"catch-up\" immediately, // placing items in configChan go ac.AddScheduler(func(configs []Config) { for _, cfg := range configs { if waiting.Load() { runtime.Gosched() configChan \u003c- cfg } } }) for len(configs) \u003c discoveryMinInstances { cfg := \u003c-configChan configs = append(configs, cfg) } return } Side note:[^1] This code is used in the Agent Check Status CLI. You will not notice resource leaks there.\nWhat Are the Issues? Utilizing an atomic Boolean as a “finished” indicator, coupled with a deferred goroutine to set it and subsequently draining a channel seems clever[^2] to me.\nUnfortunately, it has a race condition. When we first test the atomic waiting (seeing it be true), and then in parallel exit waitForConfigsFromAD, spawning the deferred goroutine which tries to drain the channel, we leak the goroutine at line 38 because no one will ever read from configChan.\nTry it on the Go Playground.\nAn Alternative Implementation Let us try a synchronous approach. The original is a little more complicated, but let’s simply assume we want to collect configurations until we either:\ncollected a fixed number. encountered a configuration error. canceled the passed context, for example in a time out. Also, we are interested in the list of encountered errors. On cancelation we just return what we have so far, without signaling an error.\nSimply put, we need a collector that allows us to wait until it is finished collecting according to the criteria above and ask what it has collected so far. Something like:\ntype Collector struct { // ... } func NewCollector(discoveryMinInstances int) *Collector { // ... } func (c *Collector) Schedule(configs []Config) { // ... } func (c *Collector) Done() \u003c-chan struct{} { // ... } func (c *Collector) Result() ([]Config, error) { // ... } Given that, we can reimplement waitForConfigsFromAD:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func waitForConfigsFromAD(ctx context.Context, discoveryMinInstances int, ac Component) ([]Config, error) { c := NewCollector(discoveryMinInstances) ac.AddScheduler(c.Schedule) select { case \u003c-ctx.Done(): case \u003c-c.Done(): } // ac.RemoveScheduler(c.Schedule) return c.Result() } Simple, synchronous code - we could even think about removing the scheduler from the component after it is done.\nNow everything else falls into place:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 type Collector struct { discoveryMinInstances int mu sync.Mutex // protects configs, errors configs []Config errors []error done chan struct{} setDone func() } func NewCollector(discoveryMinInstances int) *Collector { done := make(chan struct{}) setDone := sync.OnceFunc(func() { close(done) }) return \u0026Collector{ discoveryMinInstances: discoveryMinInstances, done: done, setDone: setDone, } } func (c *Collector) Schedule(configs []Config) { for _, cfg := range configs { if filterErrors := filterInstances(cfg); len(filterErrors) \u003e 0 { c.addErrors(filterErrors) c.setDone() continue } if !c.addConfig(cfg) { c.setDone() } } } func (c *Collector) Done() \u003c-chan struct{} { return c.done } func (c *Collector) Result() ([]Config, error) { c.mu.Lock() defer c.mu.Unlock() configs := c.configs c.configs = nil err := errors.Join(c.errors...) c.errors = nil return configs, err } func (c *Collector) addConfig(cfg Config) bool { c.mu.Lock() defer c.mu.Unlock() if len(c.configs) \u003c c.discoveryMinInstances { c.configs = append(c.configs, cfg) } return len(c.configs) \u003c c.discoveryMinInstances } func (c *Collector) addErrors(errs []error) { c.mu.Lock() defer c.mu.Unlock() if len(errs) \u003e 0 { c.errors = append(c.errors, errs...) } } There’s definite room for improvement here, but the key takeaway is that it can just be written down and it is easily testable.","summary#Summary":"We replaced asynchronous code with a race condition with a synchronous, thread safe implementation. Like in previous posts, refactoring and separation of concerns helps structuring our tasks and avoid errors."},"title":"More Concurrency Bugs"},"/posts/goroutines-1/":{"data":{"":"","concurrency-is-not-parallelism#Concurrency Is Not Parallelism":"Concurrency Is Not Parallelism[^1] Recently two interesting observations in “A Study of Real-World Data Races in Golang”[^2] caught my eye:\nObservation 1. Developers using Go employ significantly more concurrency and synchronization constructs than in Java.\nObservation 2. Developers using Go for programming microservices expose significantly more runtime concurrency than other languages such as Java, Python, and NodeJS used for the same purpose.\nThese observations agree with my experiences, and while easy concurrency is great it comes with its problems.[^3]\nIf you are familiar with the intricacies of the Go scheduler jump to part four, otherwise let’s just take a step back:\nBuilding Blocks Concurrency in Go builds upon\nGoroutines Channels Cancelation (in package context) Function literals (closures, anonymous functions) Synchronization primitives (in package sync) Result and error handling (or more generally communication between goroutines) And while it can be argued that some of these points are not exactly part of Go’s program execution model, all of these elements are identifiable in code dealing with concurrency.\nA Simple Example To start our journey, let us take a simple example: We want to calculate the 27. Fibonacci number (196,418), using a simple approach:\n1 2 3 4 5 6 7 8 9 10 11 12 package fibonacci func Slow(i int) int { if i \u003c 2 { return i } fn1 := Slow(i - 1) fn2 := Slow(i - 2) return fn1 + fn2 } And call this function 1,000 times:\n1 2 3 4 5 6 7 8 9 10 11 package main import \"fillmore-labs.com/blog/goroutines/pkg/fibonacci\" func main() { for range 1_000 { // queryStart := time.Now() _ = fibonacci.Slow(27) // duration := time.Since(queryStart) } } This is an easily understandable stand-in for a CPU-bound algorithm, so please do not send me better implementations - it is meant to burn CPU cycles.\nRunning this on a N5105 CPU gives us:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try1 *** Finished 1000 runs in 1.47s - avg 1.47ms, stddev 18.4µs So, our whole program takes 1.47 seconds on a single core. This is okay, but since the N5105 has four cores we can do better.\nLet’s parallelize (yes, I know) this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 package fibonacci func Parallel1(i int) int { if i \u003c 2 { return i } fc1 := make(chan int) go func() { fc1 \u003c- Parallel1(i - 1) }() fc2 := make(chan int) go func() { fc2 \u003c- Parallel1(i - 2) }() return \u003c-fc1 + \u003c-fc2 } Ok, great. Off we go:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try2 *** Finished 1000 runs in 5m25s - avg 325ms, stddev 6.49ms This is pretty terrible. It takes more than 200 times as long as before while using all available cores.\nOne problem is easy to spot: We are creating two new goroutines and use the original one just to wait rather than do any meaningful work. Let us fix that:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package fibonacci func Parallel2(i int) int { if i \u003c 2 { return i } fc1 := make(chan int) go func() { fc1 \u003c- Parallel2(i - 1) }() fn2 := Parallel2(i - 2) return \u003c-fc1 + fn2 } Another try, then:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try3 *** Finished 1000 runs in 1m51s - avg 111ms, stddev 3.10ms Ok, that was an easy three times speed up. Much better, but we are still much slower than the single-core solution.\nNo One Writes Code Like That Oh? containerd has a ‘concurrent’ garbage collector that spawns exponentially many goroutines:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 package gc import ( \"sync\" ) func ConcurrentMark(root Node) map[Node]struct{} { grays := make(chan Node) seen := map[Node]struct{}{} var wg sync.WaitGroup go func() { for gray := range grays { if _, ok := seen[gray]; ok { wg.Done() continue } seen[gray] = struct{}{} go func() { defer wg.Done() var children []Node = gray.Children() for _, n := range children { wg.Add(1) grays \u003c- n } }() } }() wg.Add(1) grays \u003c- root wg.Wait() close(grays) return seen } This is some clever piece of code. The access to seen is serialized through the grays channel and every Node posted to grays increments the wait group and spawns a new goroutine.\nSimply constructing a complete four-ary tree of height nine and comparing with a simple non-concurrent solution gives us the following result:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/gc Concurrent: Found 349525 reachable nodes in 462ms Non-Concurrent: Found 349525 reachable nodes in 166ms Should containerd change its solution? No. This code is seven years old and seems to be unused. The constructed test tree is pathologic and the results will be different in practice due to many already seen nodes. Also calculating children might be CPU intensive.\nThe point I am making is that the presented kind of code exists in practice and while the go scheduler is good and forgives many mistakes, it is still often not used properly.","summary#Summary":"Use of goroutines can make execution slower.\nEspecially interesting is the comparison of Parallel1 and Parallel2. While both are bad solutions, I have often seen the construct of having a goroutine exclusively for waiting instead of doing actual work, and it makes a dramatic difference in this case.\n… continued in part two."},"title":"Goroutines Are Cheap, but Not Free"},"/posts/goroutines-2/":{"data":{"":"… continued from the previous post.","back-to-the-drawing-board#Back to the Drawing Board":"Let us keep the original Slow implementation and move parallelization to main:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 package main import ( \"sync\" \"fillmore-labs.com/blog/goroutines/pkg/fibonacci\" ) func main() { var wg sync.WaitGroup for range 1_000 { // queryStart := time.Now() wg.Add(1) go func() { defer wg.Done() _ = fibonacci.Slow(27) // duration := time.Since(queryStart) }() } wg.Wait() } Now check if things improved:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try4 *** Finished 1000 runs in 371ms - avg 185ms, stddev 106ms This is approximately 3.85 times faster than the single-core solution, which is what we would expect on a four-core machine.\nWe could stop here, but some numbers immediately jump out. We measure the time between the begin of the request (the start of the goroutine) and when the calculation is done.\nWhile the single-core solution needed 1.47 milliseconds for one calculation, our latest program makes us wait on average 185 milliseconds for the result. Also, the response times vary wildly, with over 100 milliseconds of standard deviation.\nLet us diagnose why this is:\n\u003e go test -trace trace4.out fillmore-labs.com/blog/goroutines/cmd/try4 ok fillmore-labs.com/blog/goroutines/cmd/try4 0.374s \u003e go tool trace trace4.out Examining the goroutine analysis of fillmore-labs.com/blog/goroutines/cmd/try4.Run4.func1:\nWe can seed that we spawn a lot of goroutines that are are mostly waiting to be scheduled and it takes the scheduler a while to finish all of them:\nThe Go scheduler is good and has implemented some interesting prioritization tricks, so there is little penalty for this build-up, but we make scheduling bad for us and any other part of our application.\nIf we look at the block times:\nWe see the runtime calculations blocking, since we use a channel to send the duration to another goroutine, so placing unnecessary load on the scheduler affects the whole program. Also, we are just wasting RAM by creating workloads that can not be executed.\nDo Not Commission More Work Than Can Be Done Let us modify the way we schedule our calculations. Instead of submitting them all at once, we use a semaphore to only submit as many goroutines as can be executed:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 package main import ( \"context\" \"runtime\" \"fillmore-labs.com/blog/goroutines/pkg/fibonacci\" \"golang.org/x/sync/semaphore\" ) func main() { ctx := context.Background() numCPU := int64(runtime.GOMAXPROCS(0)) pool := semaphore.NewWeighted(numCPU) for range 1_000 { // queryStart := time.Now() _ = pool.Acquire(ctx, 1) go func() { defer pool.Release(1) _ = fibonacci.Slow(27) // duration := time.Since(queryStart) }() } _ = pool.Acquire(ctx, numCPU) } Code-wise it looks remarkably like the solution using wait groups. Now try this out:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try5 *** Finished 1000 runs in 372ms - avg 1.85ms, stddev 445µs It has nearly the same total runtime as our last attempt, but the requests are much more responsive (a query is calculated after 1.85 Milliseconds, which is close to our single-core result of 1.45 Milliseconds) and much less variance than before. Let us look at the trace of that:\n\u003e go test -trace trace5.out fillmore-labs.com/blog/goroutines/cmd/try5 ok fillmore-labs.com/blog/goroutines/cmd/try5 0.374s \u003e go tool trace trace5.out We see that the goroutines spend time executing code, instead of just waiting to be scheduled. Also, we have only four goroutines running at a time:\nSo, this surely is an improvement.","summary#Summary":"We studied some ways to parallelize a CPU bound algorithm so that it efficiently uses all CPU cores without swamping the Go scheduler. We also saw that randomly using goroutines has a good chance to make a program run slower than before.\nInterestingly enough the synchronous calculation of a single task is faster that the parallel one, and moving concurrency out of the calculation sped things up tremendously.\n… continued in the next post."},"title":"Using Goroutines Will Not Grant You Another CPU Core"},"/posts/goroutines-3/":{"data":{"":"… continued from the previous post.","cancelation#Cancelation":"Assume we have only a limited amount of time and want to use the data we have until this point. While we could build our own solution, but Go has context.WithTimeout since version 1.7.\nLet us modify our Fibonacci function to use a context:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 package fibonacci import ( \"context\" \"fmt\" ) func SlowCtx(ctx context.Context, i int) (int, error) { select { case \u003c-ctx.Done(): return 0, fmt.Errorf(\"fibonacci canceled: %w\", context.Cause(ctx)) default: } if i \u003c 2 { return i, nil } fn1, err1 := SlowCtx(ctx, i-1) if err1 != nil { return 0, err1 } fn2, err2 := SlowCtx(ctx, i-2) if err2 != nil { return 0, err2 } return fn1 + fn2, nil } We see some of elemts we were missing from the list of concurrency building blocks: Cancelation and error handling. Also note that checking for cancelation often will have a noticeable performance impact, we accept that for clearness and demonstration purposes.\nNow we must adapt main, too:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 package main import ( \"context\" \"runtime\" \"time\" \"fillmore-labs.com/blog/goroutines/pkg/fibonacci\" \"golang.org/x/sync/semaphore\" ) func main() { ctx := context.Background() tctx, cancel := context.WithTimeout(ctx, 100*time.Millisecond) defer cancel() numCPU := int64(runtime.NumCPU()) pool := semaphore.NewWeighted(numCPU) for range 1_000 { // queryStart := time.Now() if err := pool.Acquire(tctx, 1); err != nil { break } go func() { defer pool.Release(1) _, err := fibonacci.SlowCtx(tctx, 27) if err == nil { // duration := time.Since(queryStart) // done } else { // failed } }() } _ = pool.Acquire(ctx, numCPU) } Running this gives:\n\u003e go run fillmore-labs.com/blog/goroutines/cmd/try6 *** Finished 45 runs (4 failed) in 107ms - avg 11.1ms, stddev 3.55ms While we see a performance hit due to checking for cancelation too often and a not overly precise timer, the result is pretty satisfactory.\n\u003e go test -trace trace6.out fillmore-labs.com/blog/goroutines/cmd/try6 ok fillmore-labs.com/blog/goroutines/cmd/try6 1.403s \u003e go tool trace trace6.out Also, most goroutines are busy processing:\nand we are not blocked long waiting for other parts of the program (our runtime measurement):\nIf we modify the semaphore pool size to be 1,000 instead of runtime.NumCPU() we get results like:\n*** Finished 48 runs (952 failed) in 108ms - avg 57.8ms, stddev 30.6ms We build up lots of unnecessary goroutines in the beginning which just hang around until canceled:\nThis is also visible in the goroutine analysis:\nAnd some blocking for the few routines that manage to finish:","summary#Summary":"We introduced the concept of cancelation and error handling, so that we can limit the amount of work done when we are no longer interested in the result, for example because another part of the task failed or some deadline timed out.\n… continued in the next post."},"title":"Avoiding Unnecessary Work"},"/posts/goroutines-4/":{"data":{"":"… continued from the previous post.","problems-with-concurrency#Problems With Concurrency":"In the first post of this series I was citing two papers that examined common bugs in handling of Go concurrency.\nWe took a long time to arrive here, but I wanted to show that goroutines are not the panacea especially beginners take them for. Understanding concurrent code is sometimes hard, even when it’s easy to write.\nOne thing I see often is leaking goroutines when handling errors:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 package main import ( \"errors\" \"fmt\" \"sync\" ) func MergeContributors(primaryAccount, secondaryAccount Account) error { // Create a WaitGroup to manage the goroutines. var waitGroup sync.WaitGroup c := make(chan error) // Perform 3 concurrent transactions against the database. waitGroup.Add(3) go func() { waitGroup.Wait() close(c) }() // Transaction #1, merge \"commit\" records go func() { defer waitGroup.Done() err := mergeCommits(primaryAccount, secondaryAccount) if err != nil { c \u003c- err } }() // Transaction #2, merge \"pull request\" records go func() { defer waitGroup.Done() err := mergePullRequests(primaryAccount, secondaryAccount) if err != nil { c \u003c- err } }() // Transaction #3, merge \"merge\" records go func() { defer waitGroup.Done() err := mergePullRequestMerges(primaryAccount, secondaryAccount) if err != nil { c \u003c- err } }() // This line is bad! Get rid of it! // waitGroup.Wait() for err := range c { if err != nil { return err } } return markMerged(primaryAccount, secondaryAccount) } Can you spot the leak? Try it on the Go playground.\nThis code is adapted from “Synchronizing Go Routines with Channels and WaitGroups”. This code was not originally written by Sophie DeBenedetto and I believe she is a capable developer. It was chosen because it is pretty typical for goroutine leaks in practice, not to blame anyone personally.\nThe original code didn’t test error cases and was deadlocking if one process failed. A deadlock is probably easy to spot and will be fixed.\nWhen two ore more requests fail (which I assume is realistic, because the reason for the first failure might affect the other requests too), the first error will exit MergeContributors, the second will hang on c \u003c- err (because no one will ever read the error channel) and the first started goroutine will hang on waitGroup.Wait().\nThe resulting bug is a memory and goroutine leak and is much more intricate and easily overlooked.\nOverall, we found that there are around 42% blocking bugs caused by errors in protecting shared memory, and 58% are caused by errors in message passing. Considering that shared memory primitives are used more frequently than message passing ones, message passing operations are even more likely to cause blocking bugs.[^1]\nObservation 3. Contrary to the common belief that message passing is less error-prone, more blocking bugs in our studied Go applications are caused by wrong message passing than by wrong shared memory protection.[^1]\nAnalysis What could be considered a potential solution? One approach might use a buffered channel for errors, such as c := make(chan error, 3) (although 2 would suffice), which resolves the leak and therefore could be called a ‘fix’.\nHowever, this introduces additional magic numbers into the code, complicating its readability and maintainability. It assumes that we know the maximal number of spawned goroutines in advance, so it is more of a shotgun debugging approach rather than a comprehensive understanding and resolution of the underlying issue.\nWhat Would Be a Proper Fix? Analyzing the code, we can identify five goroutines communicating:\nThe three worker threads, doing transactions The first goroutine, waiting for the workers to finish The main goroutine (running MergeContributors) waiting for all “child” goroutines to finish and either succeed or returning the first error that happened How do they communicate? The three worker goroutines communicate with first one, the “waiter”, with a sync.WaitGroup and all four (waiter and workers) communicate with the main goroutine via the error channel.\nThis gives us an idea why the original approach of using the WaitGroup in the main goroutine is overly complicated and lead to a deadlock, resolved in the blog post.\nThe problem we can easily identify is that the main goroutine does not do what we assume, “waiting for all child goroutines to finish and either returning the first error that happened or success”. When an error occurs it abandons its duties, leaking the remaining goroutines:\n53 54 55 56 57 58 59 // waitGroup.Wait() for err := range c { if err != nil { return err } } So, let’s fix that:\n53 54 55 56 57 58 59 60 61 var err error for e := range c { if err == nil { err = e } } if err != nil { return err } This does what we expect: Using the channel close as the signal for termination and returning the first error when all goroutines are finished.\nBut This Is Bad Code Maybe. It is not tricky and does what the probable intend of the original code was. Let’s make it little bit more readable by grouping the relevant code:\n17 18 19 20 go func() { waitGroup.Wait() close(c) }() 53 54 55 56 57 58 59 60 61 var err error for e := range c { if err == nil { err = e } } if err != nil { return err } What might make the code bad is that it spawns a goroutine to do nothing but wait - it translates a wait group into a channel close.\nThe backpressure of the result channel is fine, we just don’t like it because we don’t plan to process the remaining data (errors).","summary#Summary":"Naïve use of goroutines and synchronization primitives can introduce bugs, while not necessarily improving execution efficiency.\n… continued in the next post."},"title":"Resource Leaks"},"/posts/javavirtualthreads-1/":{"data":{"":"Reading my articles about Go concurrency a friend asked me whether one could something similar in Java.","project-loom#Project Loom":"Since the release JDK 21 Java has virtual threads[^1]:\nThread.startVirtualThread(() -\u003e { System.out.println(\"Hello, world\"); }); As an equivalent to Go’s goroutines:\ngo func() { fmt.Println(\"Hello, world\") }() A Simple Example Like our experiments in Go, we implement[^2] a simple recursive calculation of the Fibonacci sequence:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 package com.fillmore_labs.blog.jvt; public final class Slow { public static int fibonacci(int n) { if (n \u003c 2) { return n; } var fn1 = fibonacci(n - 1); var fn2 = fibonacci(n - 2); return fn1 + fn2; } } Then call it 1,000 times:\n1 2 3 4 5 6 7 8 9 import com.fillmore_labs.blog.jvt.Slow; void main() { for (int i = 0; i \u003c 1_000; i++) { // var queryStart = Instant.now(); Slow.fibonacci(27); // var duration = Duration.between(queryStart, Instant.now()); } } Running this on our good old N5105 CPU gives us:\n\u003e bazel run //:try1 INFO: Running command line: bazel-bin/try1 *** Finished 1000 runs in 1.219s - avg 1.214ms, stddev 48.555µs Which is even a little faster[^3] than our Go version. Nice.\nSo, let’s try a naïve approach to parallelize things:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package com.fillmore_labs.blog.jvt; public final class Parallel1 { public static int fibonacci(int n) { if (n \u003c 2) { return n; } var ff1 = new FutureTask\u003c\u003e(() -\u003e fibonacci(n - 1)); Thread.startVirtualThread(ff1); var ff2 = new FutureTask\u003c\u003e(() -\u003e fibonacci(n - 2)); Thread.startVirtualThread(ff2); return ff1.get() + ff2.get(); } } Resulting in:\n\u003e bazel run //:try2 INFO: Running command line: bazel-bin/try2 *** Finished 1000 runs in 279.364s - avg 279.346ms, stddev 54.647ms 4 Minutes and 20 Seconds is a little better that what Go did, but still much slower than our single-threaded solution.\nAnalyzing Flame Graphs If we look at the flame graph of the single-threaded run:\n\u003e bazel run //:bench1 -- -prof \"async:output=flamegraph;direction=forward\" Iteration 1: 1220.789 ms/op Benchmark Mode Cnt Score Error Units Bench1.measure ss 1220.789 ms/op We see a little time spent interpreting/compiling the program and mostly working on our Fibonacci implementation. Our naïve implementation looks like this:\nWe spend a lot of time blocked on a Mutex in the JVM Tool Interface, maybe the global JvmtiThreadState_lock?\nOther Approaches Anyway, we are not here to debug the JVM, let’s try some other approaches.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package com.fillmore_labs.blog.jvt; import java.util.concurrent.ExecutorService; public record Parallel3(ExecutorService e) { public int fibonacci(int n) { if (n \u003c 2) { return n; } var ff1 = e.submit(() -\u003e fibonacci(n - 1)); var fn2 = fibonacci(n - 2); return ff1.get() + fn2; } } Sharing an ExecutorService and using the ‘original’ thread to do some work improves things:\n\u003e bazel run //:try3 INFO: Running command line: bazel-bin/try3 *** Finished 1000 runs in 179.452s - avg 179.426ms, stddev 41.363ms 3 Minutes is faster (interestingly enough we loose to go here) - but still slower that the single-threaded version.\nSo, let’s move parallelization to the calling function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import com.fillmore_labs.blog.jvt.Slow; import java.util.concurrent.Executors; void main() { try (var executor = Executors.newVirtualThreadPerTaskExecutor()) { for (int i = 0; i \u003c 1_000; i++) { // var queryStart = Instant.now(); executor.execute(() -\u003e { Slow.fibonacci(27); // var duration = Duration.between(queryStart, Instant.now()); }); } } } \u003e bazel run //:try4 INFO: Running command line: bazel-bin/try4 *** Finished 1000 runs in 349.151ms - avg 164.952ms, stddev 88.675ms This has a similar flame graph than the single-threaded version and is approximately 3.5 times faster.\nImprove Latency Now let us limit the number of queued calls:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 import com.fillmore_labs.blog.jvt.Slow; import java.util.concurrent.Executors; import java.util.concurrent.Semaphore; void main() throws InterruptedException { try (var executor = Executors.newVirtualThreadPerTaskExecutor()) { var numCPU = Runtime.getRuntime().availableProcessors(); var pool = new Semaphore(numCPU); for (int i = 0; i \u003c 1_000; i++) { // var queryStart = Instant.now(); pool.acquire(); executor.execute( () -\u003e { Slow.fibonacci(27); // var duration = Duration.between(queryStart, Instant.now()); pool.release(); }); } } } \u003e bazel run //:try5 INFO: Running command line: bazel-bin/try5 *** Finished 1000 runs in 359.420ms - avg 1.697ms, stddev 665.871µs Which improves our latency from 165ms to 1.7ms.","summary#Summary":"Exercises on how many threads can be started on a certain machine are mostly boring - this metric primarily showcases the small initial stack size of virtual threads.\nSeeing Java adopt virtual threads is exciting. However, it’s unlikely that Java code will resemble Go or Erlang soon. Developing correct, efficient concurrent code is much more than just replacing one threading model with another1, also there are fundamental differences in existing (standard) libraries.\n… continued in part two.\nAlan Bateman. 2023. The Challenges of Introducing Virtual Threads to the Java Platform - Project Loom — August 2023 — JVM Language Summit 2023 — \u003cyoutu.be/WsCJYQDPrrE?t=667\u003e ↩︎"},"title":"Java Virtual Threads"},"/posts/javavirtualthreads-2/":{"data":{"":"… continued from the previous post.","structured-concurrency-preview#Structured Concurrency Preview":"I’ve written about structured concurrency, and Java has a preview API[^1] StructuredTaskScope[^2]:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import com.fillmore_labs.blog.jvt.Slow2; import java.util.concurrent.StructuredTaskScope; void main() { var deadline = Instant.now().plusMillis(100L); try (var scope = new StructuredTaskScope.ShutdownOnFailure()) { for (int i = 0; i \u003c 1_000; i++) { // var queryStart = Instant.now(); scope.fork( () -\u003e { Slow2.fibonacci(27); // var duration = Duration.between(queryStart, Instant.now()); return null; }); } scope.joinUntil(deadline); } } In Java structured concurrency includes cancelation via thread interruption, aborting the unfinished calculations. We use our old recursive Fibonacci calculation as Slow2, made cancelable with:\nif (Thread.interrupted()) { throw new InterruptedException(); } When we run this, it exits after around 100 Milliseconds:\n\u003e bazel run //:try6 INFO: Running command line: bazel-bin/try6 *** Finished 129 runs (871 canceled) in 113.267ms - avg 50.995ms, stddev 16.769ms Which shows us that all virtual threads are started, even though we could only finish 129. Extending the deadline to run to completion gives:\n*** Finished 1000 runs (0 canceled) in 373.313ms - avg 172.825ms, stddev 92.234ms So, Thread.interrupted() is not free (it’s the blue areas on top), but performant enough to call it often.\nAnother Example Mirroring our Go experiments we define a task and a function calling it:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 import java.time.Duration; import java.time.Instant; import java.util.concurrent.StructuredTaskScope; final Duration processingTime = Duration.ofSeconds(1); void main() throws Exception { try (var scope = new StructuredTaskScope.ShutdownOnFailure()) { var start = Instant.now(); scope.fork( () -\u003e { task(\"task1\", processingTime.dividedBy(3), null); return null; }); scope.fork( () -\u003e { task(\"task2\", processingTime.dividedBy(2), new TestException(\"task2 failed\")); return null; }); scope.fork( () -\u003e { task(\"task3\", processingTime, null); return null; }); scope.join(); var result = scope.exception(); var duration = Duration.between(start, Instant.now()); System.out.println(STR.\"*** Got \\\"\\{result}\\\" in \\{duration}\"); } } void task(String name, Duration processingTime, Exception result) throws Exception { Thread.sleep(processingTime); if (result != null) { throw result; } } static class TestException extends Exception { TestException(String message) { super(message); } } Running this, we see similar results as in our previous experiment:\n\u003e bazel run //:try7 INFO: Running command line: bazel-bin/try7 *** Got \"com.fillmore_labs.blog.jvt.TestException: task2 failed\" in 520,398ms So ShutdownOnFailure closely mimics Go’s errgroup.","summary#Summary":"Java seems to bet on structured concurrency, at least for virtual threads in non-library code. It uses thread interruption as a means of cancelation, which requires having a handle to the running thread. We might eventually see support for context propagation, e.g. from OpenTelemetry, for the new constructs. This is conceptually very different for Go’s context, which is just hierarchically passed down and cancels tasks, including subtasks, regardless of whether the canceler is aware of them."},"title":"Java Structured Concurrency"},"/posts/structured-1/":{"data":{"":"","#":"… continued from the previous post.\nDifferent Categories of Concurrency When recalling the previous article, a task was subdivided into three subtasks, all working towards a common objective (specifically, merging contributors).\nThe subtasks are started in the main task and reach completion, yielding results, within the lifespan of that overarching task.\nThis pattern was named “Structured Concurrency” by Martin Sústrik[^1] and further examined in Nathaniel J. Smith’s “Notes on structured concurrency, or: Go statement considered harmful”[^2].\nWhile I do not subscribe to every viewpoint expressed in these articles, I believe that this is at least a valid concurrency pattern. Also, it’s well-suited to Go’s hierarchical contexts and cancelation mechanisms.\nGroundwork Let’s start with a typical subtask\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package task import ( \"context\" \"fmt\" \"time\" ) func Task(ctx context.Context, name string, processingTime time.Duration, result error) error { ready := time.NewTimer(processingTime) select { case \u003c-ctx.Done(): ready.Stop() fmt.Println(name, ctx.Err()) return fmt.Errorf(\"%s canceled: %w\", name, ctx.Err()) case \u003c-ready.C: fmt.Println(name, result) } return result } We define a task.Task as a dummy workload, having a name as an identity and a processingTime after which it finishes.\nThe task has two properties that are important:\nIt has a synchronous API and returns an error It takes a context.Context parameter and exits early when the context is canceled. Having a context is especially important so that we don’t perform a lot of work which is irrelevant in case the overarching task already failed.\nConsider this scenario: You make a query to a service that fails to respond. Eventually, a higher-level context times out, and it is important for your function to terminate to prevent any potential resource leakage.\nWhen a request is canceled or times out, all the goroutines working on that request should exit quickly so the system can reclaim any resources they are using.[^3]\nContext We define an doWork function which takes a higher-level context and distributes the work over three subtasks.\nFirst, we create a sub-context of the passed context which will be canceled when we leave the scope. We pass this context to all created goroutines, ensuring we don’t leak resources.\nAdditionally, when we can’t complete our task (because a subtask failed) we cancel the remaining work so we can return early.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 package main import ( \"context\" \"fillmore-labs.com/blog/structured/pkg/task\" ) func doWork(ctx context.Context) error { ctx, cancel := context.WithCancelCause(ctx) defer cancel(nil) var g int errc := make(chan error) g++ go func() { errc \u003c- task.Task(ctx, \"task1\", processingTime/3, nil) }() g++ go func() { errc \u003c- task.Task(ctx, \"task2\", processingTime/2, errFail) }() g++ go func() { errc \u003c- task.Task(ctx, \"task3\", processingTime, nil) }() var err error for range g { if e := \u003c-errc; e != nil \u0026\u0026 err == nil { err = e cancel(err) } } return err } Since we fail task 2 we expect the following call sequence:\nsequenceDiagram participant Main create participant Work Main-\u003e\u003eWork: go Work(main ctx) Note right of Work: Create work ctx create participant Task1 Work-\u003e\u003eTask1: go Task1(work ctx) create participant Task2 Work-\u003e\u003eTask2: go Task2(work ctx) create participant Task3 Work-\u003e\u003eTask3: go Task3(work ctx) Note over Task1: Task1 completes destroy Task1 Task1-\u003e\u003eWork: no error (“nil”) Note over Task2: Task2 completes destroy Task2 Task2-\u003e\u003eWork: failed Note over Task3: Task3 processing Note right of Work: First error cancels work ctx Work--)Task3: cancel work ctx Note over Task3: Task3 interrupted destroy Task3 Task3-\u003e\u003eWork: canceled Note right of Work: All subtaks complete destroy Work Work-\u003e\u003eMain: failed Running Multiple Subtasks Concurrently So, we run a number of subtasks in goroutines, simply counting them with g++ - which is easier to track than having a magic 3 at the top - and collect all results in the end. This is pretty simple code, and when we run it we get the expected result:\n\u003e go run fillmore-labs.com/blog/structured/cmd/structured1 task1 \u003cnil\u003e task2 failed task3 context canceled Got \"failed\" error in 501ms Try it on the Go Playground.\nAlso, we see that the task returns nearly immediately after the first failure (500 milliseconds) and doesn’t let the third subtask unnecessarily consume resources.\nConsidering the possibility of “optimization”, where we could return the error immediately without awaiting the completion of canceled subtasks: Since we expect canceled subtasks to return quickly (they can just abort), we might save very little time in error scenarios, without any gain in normal processing. Compared to the risk of a resource leak this appears to be a bad tradeoff.","summary#Summary":"Structured Concurrency is a pattern that is useful in writing correct Go programs. A context parameter und creating sub-contexts is helpful for avoiding resource leaks.\n… continued in the next post."},"title":"Structured Concurrency"},"/posts/structured-2/":{"data":{"":"","do-not-communicate-by-sharing-memory-instead-share-memory-by-communicating#Do Not Communicate by Sharing Memory; Instead, Share Memory by Communicating":"… continued from the previous post.\nDo Not Communicate by Sharing Memory; Instead, Share Memory by Communicating[^1] This approach can be taken too far. […] But as a high-level approach, using channels to control access makes it easier to write clear, correct programs.[^1]\nLet’s just for comparison reformulate the scheme in the last post with shared variables and a sync.WaitGroup:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 package main import ( \"context\" \"sync\" \"fillmore-labs.com/blog/structured/pkg/task\" ) func doWork(ctx context.Context) error { ctx, cancel := context.WithCancelCause(ctx) defer cancel(nil) var firstErr error var once sync.Once setErr := func(err error) { if err == nil { return } once.Do(func() { firstErr = err cancel(err) }) } var wg sync.WaitGroup wg.Add(1) go func() { defer wg.Done() err := task.Task(ctx, \"task1\", processingTime/3, nil) setErr(err) }() wg.Add(1) go func() { defer wg.Done() err := task.Task(ctx, \"task2\", processingTime/2, errFail) setErr(err) }() err := task.Task(ctx, \"task3\", processingTime, nil) setErr(err) wg.Wait() return firstErr } Here we replace the error channel with a function storing the first error and canceling the context. This works fine:\n\u003e go run fillmore-labs.com/blog/structured/cmd/structured2 task1 \u003cnil\u003e task2 failed task3 context canceled Got \"failed\" error in 501ms But it is a lot of boilerplate.\nRefactor and Separate Concerns We can easily extract the orchestration part:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 package main import ( \"context\" \"sync\" ) type Group struct { err error cancel context.CancelCauseFunc once sync.Once wg sync.WaitGroup } func NewGroup(cancel context.CancelCauseFunc) *Group { return \u0026Group{cancel: cancel} } func (g *Group) Do(fn func() error) { err := fn() if err == nil { return } g.once.Do(func() { g.err = err if g.cancel != nil { g.cancel(err) } }) } func (g *Group) Go(fn func() error) { g.wg.Add(1) go func() { defer g.wg.Done() g.Do(fn) }() } func (g *Group) Wait() error { g.wg.Wait() return g.err } Making our function only:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main import ( \"context\" \"fillmore-labs.com/blog/structured/pkg/task\" ) func doWork(ctx context.Context) error { ctx, cancel := context.WithCancelCause(ctx) defer cancel(nil) g := NewGroup(cancel) g.Go(func() error { return task.Task(ctx, \"task1\", processingTime/3, nil) }) g.Go(func() error { return task.Task(ctx, \"task2\", processingTime/2, errFail) }) g.Do(func() error { return task.Task(ctx, \"task3\", processingTime, nil) }) return g.Wait() } This separates processing and orchestration, which is nice and makes our code much more readable and improves testability.","summary#Summary":"Separating orchestration from the processing code, we can reach simplified structured concurrency with improved readability while eliminating some sources of resource leaks.\n… continued in the next post."},"title":"An Alternative Approach"},"/posts/structured-3/":{"data":{"":"","#":"… continued from the previous post.\nRefactor Our Original Approach What we have done to the previously can also be done to the approach using an error channel:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 package main import \"context\" type Group struct { errc chan error cancel context.CancelCauseFunc count int } func NewGroup(cancel context.CancelCauseFunc) *Group { return \u0026Group{errc: make(chan error, 1), cancel: cancel} } func (g *Group) Go(f func() error) { g.count++ go func() { g.errc \u003c- f() }() } func (g *Group) Wait() error { var err error for range g.count { if e := \u003c-g.errc; e != nil \u0026\u0026 err == nil { err = e g.cancel(e) } } return err } Making our function:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 package main import ( \"context\" \"fillmore-labs.com/blog/structured/pkg/task\" ) func doWork(ctx context.Context) error { ctx, cancel := context.WithCancelCause(ctx) defer cancel(nil) g := NewGroup(cancel) g.Go(func() error { return task.Task(ctx, \"task1\", processingTime/3, nil) }) g.Go(func() error { return task.Task(ctx, \"task2\", processingTime/2, errFail) }) g.Go(func() error { return task.Task(ctx, \"task3\", processingTime, nil) }) return g.Wait() } As we see we get a nearly identical result for the main function, witht the API neatly abstracting our soulution. One difference is that we have to call all subtasks asynchronously, since we need Group.Wait working on the error channel.","summary#Summary":"We have seen two approaches to structured concurrency with nearly identical APIs.\n… continued in the next post."},"title":"Comparison to Our Original Approach"},"/posts/structured-4/":{"data":{"":"","#":"… continued from the previous post.\nTwo Popular Choices Perhaps the most popular exisiting library is golang.org/x/sync/errgroup by Bryan C. Mills:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \"context\" \"fillmore-labs.com/blog/structured/pkg/task\" \"golang.org/x/sync/errgroup\" ) func doWork(ctx context.Context) error { g, ctx := errgroup.WithContext(ctx) g.Go(func() error { return task.Task(ctx, \"task1\", processingTime/3, nil) }) g.Go(func() error { return task.Task(ctx, \"task2\", processingTime/2, errFail) }) g.Go(func() error { return task.Task(ctx, \"task3\", processingTime, nil) }) return g.Wait() } and the older gopkg.in/tomb.v2 by Gustavo Niemeyer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \"context\" \"fillmore-labs.com/blog/structured/pkg/task\" \"gopkg.in/tomb.v2\" ) func doWork(ctx context.Context) error { g, ctx := tomb.WithContext(ctx) g.Go(func() error { return task.Task(ctx, \"task1\", processingTime/3, nil) }) g.Go(func() error { return task.Task(ctx, \"task2\", processingTime/2, errFail) }) g.Go(func() error { return task.Task(ctx, \"task3\", processingTime, nil) }) return g.Wait() } ","summary#Summary":"In practical scenarios, where numerous goroutines are initiated, structured concurrency guarantees their proper management without leaking resources. The managing goroutine must not exit until all its child goroutines complete. Additionally, structured concurrency guarantees thorough error reporting, preventing any errors from being overlooked or disregarded.\n… continued in the next post."},"title":"Existing Libraries"},"/posts/structured-5/":{"data":{"":"… continued from the previous post.\nShuffling through Stack Overflow questions I realized that there is one point I tried to make clear, but didn’t emphasize enough:","be-considerate#Be Considerate":"I do not believe everything should be written using structured concurrency. But seeing common Go bugs, I think that a\nlot of code written would benefit.","problems-in-the-wild#Problems in the Wild":"Let me give some examples. It’s so much better to have:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func task() int { x := subX() y := subY(x) return y } func subX() int { return 1 } func subY(i int) int { return i + 1 } And realize - well, you can’t make it concurrent, because one function depends on the result of the other, and besides - it is fast enough - instead of being stuck with Frankenstein’s monster:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 func task() int { xy := make(chan int) result := make(chan int) go subX(xy) go subY(xy, result) return \u003c-result } func subX(out chan\u003c- int) { out \u003c- 1 } func subY(in \u003c-chan int, out chan\u003c- int) { out \u003c- \u003c-in + 1 } which is slower than the first example.\nAnd it is also so much easier instead of guessing what should be concurrent up front to start with synchronous code:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 func task() (int, error) { s1, err := sub1() if err != nil { return 0, err } s2, err := sub2() if err != nil { return 0, err } return s1 + s2, nil } func sub1() (int, error) { return 1, nil } func sub2() (int, error) { return 1, nil } and should you find out that executing sub1 and sub2 concurrently could speeds thing up, transform it to:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 func task() (int, error) { var s1, s2 int var g errgroup.Group g.Go(func() (err error) { s1, err = sub1() return err }) g.Go(func() (err error) { s2, err = sub2() return err }) err := g.Wait() if err != nil { return 0, err } return s1 + s2, nil } func sub1() (int, error) { return 1, nil } func sub2() (int, error) { return 1, nil } What makes things much better here is that sub1 and sub2 still have unchanged, synchronous APIs, which means that all tests you’ve written are still valid and things are much easier to test, since you don’t have to deal with concurrency.\nThe transformation only happens in task, and at that scope concurrency is better to understand.","summary#Summary":"Most of the code should be written synchronously first and should keep synchronous APIs as much as possible. Function literals are a great way to separate concurrency from subtasks.\n… to be continued.","write-synchronous-code-first#Write Synchronous Code First":"Many programs work perfectly fine without concurrency. It’s better to prioritize creating a functional and thoroughly tested program initially and introduce concurrency when its benefits become apparent through observation of runtime behavior. Rushing into concurrent implementations riddled with bugs invariably results in more time spent on subsequent fixes of a bad design and less on real improvement; moreover, it’s more efficient to start with a straightforward approach and iterate towards improvement, rather than to deal with a buggy program and invest significant time in bug fixes afterward."},"title":"How to Write Concurrent Go Code"},"/posts/zerosized-1/":{"data":{"":"Imagine writing a translation function that transforms internal errors into public API errors. In the first iteration, you return nil when no translation takes place. You make a simple change — returning the original error instead of nil — and suddenly your program behaves differently:\ntranslate1: unsupported operation translate2: internal not implemented This program prints two different results from the nearly identical functions (Go Playground). Can you spot why?\n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 type NotImplementedError struct{} func (*NotImplementedError) Error() string { return \"internal not implemented\" } func Translate1(err error) error { if (err == \u0026NotImplementedError{}) { return errors.ErrUnsupported } return nil } func Translate2(err error) error { if (err == \u0026NotImplementedError{}) { return errors.ErrUnsupported } return err } func main() { fmt.Printf(\"translate1: %v\\n\", Translate1(DoWork())) fmt.Printf(\"translate2: %v\\n\", Translate2(DoWork())) } func DoWork() error { return \u0026NotImplementedError{} } The key to this divergent behavior lies in NotImplementedError being a zero-sized type (ZST). ZSTs are types that occupy no memory. Common examples include empty structs struct{} or a zero-length array [0]byte. While highly useful for tasks like signaling without data transfer (e.g., (chan struct{} in Context.Done()) or implementing sets (map[string]struct{}), their interaction with pointers can be particularly tricky.\nSo, why does this simple change lead to different outcomes?","a-new-linter-of-hope#A New Linter of Hope":"zerolint is a static analysis tool specifically designed to detect problematic usage patterns involving zero-sized types in Go. For example, it will flag the following:\ngo install fillmore-labs.com/zerolint@latest zerolint . /path/to/your/project/main.go:10:7: error interface implemented on pointer to zero-sized type \"example.com/project.NotImplementedError\" (zl:err) /path/to/your/project/main.go:27:6: comparison of pointer to zero-size type \"example.com/project.NotImplementedError\" with error interface (zl:cme) /path/to/your/project/main.go:35:6: comparison of pointer to zero-size type \"example.com/project.NotImplementedError\" with error interface (zl:cme) Preventing Pointer Pandemonium This tool helps avoid the errors discussed by catching potential issues before they lead to runtime bugs, saving debugging time and preventing unexpected behavior in production.\nIdentifying Unreliable Comparisons: The linter flags comparisons of pointers to zero-sized types, such as errors.Is(err, \u0026NotImplementedError{}). By highlighting these comparisons during development or in CI/CD pipelines, zerolint prompts developers to reconsider their logic. Instead of relying on pointer equality (which is unspecified for distinct ZSTs), developers are encouraged to use sentinel errors or reconsider whether a value type is more appropriate.\nDiscouraging Pointer Receivers for ZSTs: The linter warns when methods are defined on pointer receivers for zero-sized types (e.g., func (*NotImplementedError) Error() string). While syntactically valid, using pointer receivers for ZSTs is often unnecessary and can lead to the kind of pointer-related confusion the article describes. Value receivers (func (NotImplementedError) Error() string) are usually clearer and avoid any ambiguity about pointer identity for ZSTs, as comparisons of values of these types (not pointers to them) are straightforward. zerolint encourages this safer practice.\nPromoting Idiomatic Go: By flagging these patterns, zerolint helps enforce idioms that lead to more robust and maintainable Go code. It steers developers away from relying on implementation details or unspecified behaviors of the compiler and runtime concerning ZST pointers.\nThe kinds of bugs arising from ZST pointer comparisons can be subtle and hard to reproduce, as they might depend on compiler versions or specific build conditions. zerolint catches these potential issues before they lead to runtime bugs.\nIn essence, zerolint helps developers adhere to best practices, such as those discussed next regarding struct embedding and those summarized in this post’s conclusion.","pointergeist-the-zero-dimension#Pointergeist: The Zero Dimension":"To understand this behavior, let’s see what the Go compiler does behind the scenes.\nThe Great Pointer Escape When we run go build -gcflags='-m=1' ., we get something like:\n... ./main.go:14:17: err does not escape ./main.go:15:13: \u0026NotImplementedError{} does not escape ./main.go:22:17: leaking param: err to result ~r0 level=0 ./main.go:23:13: \u0026NotImplementedError{} does not escape ./main.go:31:12: ... argument does not escape ./main.go:31:50: \u0026NotImplementedError{} does not escape ./main.go:31:43: \u0026NotImplementedError{} does not escape ./main.go:32:12: ... argument does not escape ./main.go:32:50: \u0026NotImplementedError{} escapes to heap ./main.go:32:43: \u0026NotImplementedError{} does not escape ./main.go:36:9: \u0026NotImplementedError{} escapes to heap The critical line is \u0026NotImplementedError{} escapes to heap - this refers to the pointer created in the call to DoWork() in line 32. It escapes because the parameter in line 22 is a potential return value (return err), which is fed into fmt.Errorf.\nEscape analysis determines whether variables can remain on the stack or must be allocated on the heap. When the compiler can’t prove a variable won’t outlive its function, it “escapes” to the heap.\nHoney, I Shrunk the Alloc When Go allocates memory for zero-sized types, it applies a clever optimization: the runtime returns a pointer to runtime.zerobase (see the mallocgc implementation), a single static variable shared by all heap-allocated ZSTs, instead of allocating new memory for each instance.\nThis means multiple \u0026NotImplementedError{} instances that escape to the heap typically point to the same address.\nOn the other hand, the \u0026NotImplementedError{} in the comparison in Translate2 does not escape. It gets the address of a (zero-sized) variable on the stack.\nSo in Translate2, the comparison err == \u0026NotImplementedError{} involves two pointers that, due to one being on the stack and the other pointing to runtime.zerobase, have different memory addresses, thus evaluating to false.\nBy Design: The Equality Paradox In Translate1, since the err parameter (which is another instance of \u0026NotImplementedError{}) does not escape within this function, and the \u0026NotImplementedError{} literal in the comparison also doesn’t escape, both are effectively stack-scoped. For stack-allocated ZSTs, the compiler may optimize by assigning them the same memory address, but this is not guaranteed. This contributes to the unreliability of direct pointer comparisons for ZSTs, as the outcome can vary depending on compiler decisions and optimizations.\nThis is not a bug — the Go language specification explicitly states that the equality of pointers to distinct zero-size variables is unspecified:\n“pointers to distinct zero-size variables may or may not be equal.”\nDepending on the path the compiler takes, our program’s behavior changes, making our logic unsound. The bug is in the program above, not the compiler.\nYou can easily validate the issue by making the error type non-zero-sized:\n8 type NotImplementedError struct{ _ int } The errors.Is Illusion: Escaped Zero-Sized Types All Look the Same I can hear you saying “Yes, but you should compare errors with errors.Is.” You absolutely should. The example becomes a little more complex (Go Playground):\n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 type NotImplementedError struct{} func (*NotImplementedError) Error() string { return \"internal not implemented\" } func Translate(err error) error { if errors.Is(err, \u0026NotImplementedError{}) { return errors.ErrUnsupported } return err } func main() { fmt.Printf(\"translate1: %v\\n\", Translate(DoWork1())) fmt.Printf(\"translate2: %v\\n\", Translate(DoWork2())) } func DoWork1() error { return \u0026NotImplementedError{} } func DoWork2() error { n := struct { NotImplementedError _ int // Makes this a non-zero-sized type }{} return \u0026n.NotImplementedError // Returns pointer to embedded ZST } It’s important to understand that internally errors.Is first attempts a direct pointer comparison — the same pointer comparison as above when comparing concrete error values:\n“An error is considered to match a target if it is equal to that target”\nArguments passed to functions that are not fully inlinable (like errors.Is), or whose lifetime cannot be determined at compile time to be confined to the caller’s stack, will usually escape to the heap. As mentioned earlier, the Go runtime frequently optimizes them to point to a single static address.\nThis makes errors.Is work more predictably for heap-allocated ZST pointers, but you’re still relying on an implementation detail rather than guaranteed behavior.","practical-application-the-art-of-embedding#Practical Application: The Art of Embedding":"Let’s look at some common scenarios, like struct embedding, where these principles and a tool like zerolint become particularly relevant.\nValue Embedding: Saving Private Memory (With Free Methods) Zero-sized defined types can group a set of methods for a default behavior or interface implementation.\nA method call on a zero-sized receiver has the same runtime behavior as a pure function call, since there’s no instance data to pass on the stack. Passing a ZST as a parameter is essentially free at runtime. This is different for pointer, though, as we will see below.\nAssuming we want to have a variant of afero.OsFs with the Remove method disabled, we would do:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \"errors\" \"fmt\" \"unsafe\" \"github.com/spf13/afero\" ) var ErrInvalidOperation = errors.New(\"invalid operation\") type ReadOnlyFS struct { afero.OsFs // Embedded value - 0 bytes overhead } func (ReadOnlyFS) Remove(name string) error { return ErrInvalidOperation } func main() { a := ReadOnlyFS{} fmt.Println(unsafe.Sizeof(a.OsFs)) // Prints: 0 _, _ = a.Open(\"test.txt\") // Works perfectly } This prints 0, demonstrating that we get all the methods from afero.OsFs with zero memory overhead.\nCommon Pitfall: The Eight-Byte Burden \u0026 The Nil Panic Now let’s see what happens when we use pointers instead:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 package main import ( \"errors\" \"fmt\" \"unsafe\" \"github.com/spf13/afero\" ) var ErrInvalidOperation = errors.New(\"invalid operation\") type ReadOnlyFS struct { *afero.OsFs // Embedded pointer - 8 bytes + nil pointer risk } func (*ReadOnlyFS) Remove(name string) error { return ErrInvalidOperation } func main() { a := \u0026ReadOnlyFS{} fmt.Println(unsafe.Sizeof(a.OsFs)) // Prints: 8 (wasted space) _, _ = a.Open(\"test.txt\") // Panics! (nil pointer) } This approach has two problems:\nFirst, adding a level of indirection to zero-sized types negates their primary memory benefit. A pointer to a ZST itself consumes memory (typically 8 bytes on a 64-bit system for the pointer itself), whereas the ZST value consumes zero bytes. Since the pointer could mostly be the address of runtime.zerobase or nil, we waste 8 bytes for very little information.\nSecond, the embedded pointer OsFs is nil because it’s uninitialized. While Go allows calling methods on nil receivers if the method is defined to handle nil (e.g., by checking if the receiver is nil), in this case, a.Open(\"test.txt\") attempts to dereference the nil pointer (as Open has a value receiver), causing a runtime panic.\nInterface Evolution: gRPC-Go Case Study ZSTs are excellent for providing default implementations for interfaces, especially when embedded by value. Since an embedded ZST occupies 0 bytes, it adds no size to the surrounding struct, so there’s no overhead.\nA prominent example of this is found in gRPC for Go, specifically in server implementations. To ensure forward compatibility, UnimplementedXXXServer structs are provided:\ngenerated.go// UnimplementedXXXServer must be embedded to have // forward compatible implementations. // // NOTE: this should be embedded by value instead of pointer to avoid a nil // pointer dereference when methods are called. type UnimplementedXXXServer struct{} func (UnimplementedXXXServer) GetUser(context.Context, *GetUserRequest) (*GetUserResponse, error) { return nil, status.Errorf(codes.Unimplemented, \"method GetUser not implemented\") } implementation.gotype XXXServer struct { UnimplementedXXXServer } func (XXXServer) GetUser(context.Context, *GetUserRequest) (*GetUserResponse, error) { ... } For some background see gRPC-Go’s issue 2318 and issue 3669.","the-tao-of-zst-a-guide-to-robust-go#The Tao of ZST: A Guide to Robust Go":"Zero-sized types (ZSTs) are a Go feature offering memory efficiency and powerful design mechanisms. However, as we’ve seen, their interaction with pointers can lead to subtle, hard-to-diagnose bugs if not handled with care. Relying on specific compiler or runtime behaviors (like all escaped ZST pointers resolving to runtime.zerobase) for pointer comparisons is a path to fragile code.\nTo harness the benefits of ZSTs while avoiding the pitfalls, keep these core principles in mind:\nEmbrace Values, Eschew Pointers: Whenever possible, use ZSTs as direct values (e.g., myZST) rather than pointers to them (\u0026myZST). This approach completely avoids the ambiguity of ZST pointer comparisons.\nValue Receivers are Preferred: Define methods on value receivers (e.g., func (z MyZST) Method()) for ZSTs. Pointer receivers (e.g., func (z *MyZST) Method()) are rarely necessary for ZSTs and can reintroduce issues with pointer comparisons.\nEmbed Values, Not Pointers: When composing structs, embed ZSTs directly (e.g., embed MyZST) to maintain their zero memory overhead benefit. Embedding pointers to ZSTs (embed *MyZST) negates this by adding pointer overhead and the risk of nil pointers.\nBe Mindful of ZSTs When Designing API Contracts: For example, use functional options instead of pointer-based configuration for zero-sized option structs.\nMind API Boundaries: The primary legitimate use case for pointers to ZSTs arises when an external API you must interact with explicitly requires a pointer receiver or argument. In such instances, the choice is driven by compatibility.\nLeverage Static Analysis: You don’t have to navigate these nuances alone. Tools like zerolint are invaluable for automatically flagging problematic ZST pointer usage in your codebase, guiding you towards safer and more idiomatic Go practices.\nThe key insight is that zero-sized types are powerful as values but risky and wasteful as pointers. Understanding this distinction will save you from subtle, hard-to-debug issues that can vary between compiler versions and build conditions.\n… continued in the next post."},"title":"The Perils of Pointers in the Land of the Zero-Sized Type"},"/posts/zerosized-2/":{"data":{"":"… continued from the previous post.\nWhile researching the usage of zero-sized types in Go I wrote zerolint1 and the accompanying cmplint2 and examined over 500 popular Go projects.\nI want to look into some examples why I think those linters are useful. Let’s start with golang.org/x/sync/singleflight:\ngit clone --branch v0.15.0 https://go.googlesource.com/sync golang-sync cd golang-sync zerolint ./singleflight # .../golang-sync/singleflight/singleflight_test.go:24:11: error interface implemented on pointer to zero-sized type \"golang.org/x/sync/singleflight.errValue\" (zl:err) # .../golang-sync/singleflight/singleflight_test.go:78:8: comparison of pointer to zero-size type \"golang.org/x/sync/singleflight.errValue\" with error interface (zl:cme) Okay, interesting. Let’s see:\nsingleflight_test.go 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 package singleflight import ... type errValue struct{} func (err *errValue) Error() string { return \"error value\" } func TestPanicErrorUnwrap(t *testing.T) { t.Parallel() testCases := []struct { name string panicValue interface{} wrappedErrorType bool }{ { name: \"panicError wraps non-error type\", panicValue: \u0026panicError{value: \"string value\"}, wrappedErrorType: false, }, { name: \"panicError wraps error type\", panicValue: \u0026panicError{value: new(errValue)}, wrappedErrorType: false, }, } for _, tc := range testCases { tc := tc t.Run(tc.name, func(t *testing.T) { t.Parallel() var recovered interface{} group := \u0026Group{} func() { defer func() { recovered = recover() t.Logf(\"after panic(%#v) in group.Do, recovered %#v\", tc.panicValue, recovered) }() _, _, _ = group.Do(tc.name, func() (interface{}, error) { panic(tc.panicValue) }) }() if recovered == nil { t.Fatal(\"expected a non-nil panic value\") } err, ok := recovered.(error) if !ok { t.Fatalf(\"recovered non-error type: %T\", recovered) } if !errors.Is(err, new(errValue)) \u0026\u0026 tc.wrappedErrorType { t.Errorf(\"unexpected wrapped error type %T; want %T\", err, new(errValue)) } }) } } This test should verify that\nA panic in a Group.Do is caught and re-thrown in the enclosing scope.\nThe value returned from recover is always an error type.\nIf the value passed to panic is also an error, the result from recover wraps the value.\nThe last point is why this test is named TestPanicErrorUnwrap.\nLet us clean up a little before we come to the point:\nThe test cases are erroneous. wrappedErrorType is always false (should be true in the second case) and the values are already wrapped in the private error (they shouldn’t):\n36 37 38 39 40 41 42 43 44 45 { name: \"panicError wraps non-error type\", panicValue: \"string value\", wrappedErrorType: false, }, { name: \"panicError wraps error type\", panicValue: new(errValue), wrappedErrorType: true, }, We should probably change the package to singleflight_test and import . \"golang.org/x/sync/singleflight\" to avoid using internal types. Skip this when you believe dot imports are evil.\nLine 65 tc := tc is not needed since Go 1.223\nAlso, line 79 should be something like t.Errorf(\"unexpected wrapped error \\\"%v\\\"; want \\\"%v\\\"\", err, new(errValue)), since the error type we get is internal, so we can’t directly expect that - it should only wrap our error.\nLet’s run the modified test (Go Playground):\n=== RUN TestPanicErrorUnwrap === RUN TestPanicErrorUnwrap/panicError_wraps_non-error_type === RUN TestPanicErrorUnwrap/panicError_wraps_error_type --- PASS: TestPanicErrorUnwrap (0.00s) --- PASS: TestPanicErrorUnwrap/panicError_wraps_non-error_type (0.00s) --- PASS: TestPanicErrorUnwrap/panicError_wraps_error_type (0.00s) PASS Fine, we are done? Just for the fun of it, let’s modify line 10 to type errValue struct{ _ int } and run it again (Go Playground):\n=== RUN TestPanicErrorUnwrap === RUN TestPanicErrorUnwrap/panicError_wraps_non-error_type === RUN TestPanicErrorUnwrap/panicError_wraps_error_type --- FAIL: TestPanicErrorUnwrap (0.00s) --- PASS: TestPanicErrorUnwrap/panicError_wraps_non-error_type (0.00s) --- FAIL: TestPanicErrorUnwrap/panicError_wraps_error_type (0.00s) FAIL So, when you read the last article you’ll know that errors.Is(err, new(T)) is undefined for zero-sized Ts. Let us first fix the test:\nUse a (local) sentinel value errValue := errors.New(\"error value\") instead of the type.\nReplace all new(errValue) by references to the sentinel errValue.\nThe fixed test4 runs (Go Playground), is correct, not flagged by zerolint and easier to understand.\nAs a closing remark: zerolint (and in this case cmplint) find real errors in popular Go programs that are not necessarily detectable by testing.\n… to be continued.\nfillmore-labs.com/zerolint ↩︎\nfillmore-labs.com/cmplint ↩︎\nIn Go 1.22 Go changed the semantics of for loop variables. ↩︎\ngolang.org/cl/685736 ↩︎"},"title":"A Zero-Sized Bug Hunt in golang.org/x/sync"}}